{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77aff34-b9da-4a89-bec8-3bafaa881bf5",
   "metadata": {},
   "source": [
    "### connect to spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b7b2a5-4518-4ec2-a137-ac7c0576c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/14 15:06:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import RDD\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"PatrickHennig_Lab3\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# too verbose..\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b95be14-618e-4559-b7f7-0d429a5f53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bb235-8677-484c-b88b-d4979121c2a1",
   "metadata": {},
   "source": [
    "spark RDD documentation https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45440d-8160-4f1a-a391-258f6fa13775",
   "metadata": {},
   "source": [
    "## Question A.1\n",
    "\n",
    "A.1.1 Read the English transcripts with Spark, and count the number of lines.\n",
    "\n",
    "A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for\n",
    "each).\n",
    "\n",
    "A.1.3 Verify that the line counts are the same for the two languages.\n",
    "\n",
    "A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2eaf46-1c4f-4241-9f84-f263e4221a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines in the german version: 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines in the english version: 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num partitions in the german set: 3\n",
      "num partitions in the english set: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# .textFile() reads into RDD containing individual lines\n",
    "\n",
    "# A.1.1\n",
    "lines_de = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.de-en.de\")\n",
    "print(\"lines in the german version:\",lines_de.count())\n",
    "\n",
    "# A.1.2\n",
    "lines_en = spark_context.textFile(\"hdfs://192.168.2.250:9000/europarl/europarl-v7.de-en.en\")\n",
    "print(\"lines in the english version:\",lines_en.count())\n",
    "\n",
    "# A.1.3\n",
    "assert lines_de.count()==lines_en.count(),f\"number of lines differ between english and german (en:{lines_en.count()}, de:{lines_de.count()})\"\n",
    "\n",
    "# A.1.4\n",
    "print(\"num partitions in the german set:\",lines_de.getNumPartitions())\n",
    "print(\"num partitions in the english set:\",lines_en.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddc7b0-a7d4-4f09-bd87-980a89d25dc0",
   "metadata": {},
   "source": [
    "# Question A.2 #\n",
    "    \n",
    "A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    " - Lowercase the text\n",
    " - Tokenize the text (split on space)\n",
    "   \n",
    "Hint: define a function to run in your driver application to avoid writing this code twice.\n",
    "\n",
    "A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing.\n",
    "\n",
    "A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31e1550-a728-4182-a16d-7fd1c3ecfb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words in the german version: 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words in the english version: 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wiederaufnahme', 'der', 'sitzungsperiode'], ['ich', 'erkläre', 'die', 'am', 'freitag,', 'dem', '17.', 'dezember', 'unterbrochene', 'sitzungsperiode', 'des', 'europäischen', 'parlaments', 'für', 'wiederaufgenommen,', 'wünsche', 'ihnen', 'nochmals', 'alles', 'gute', 'zum', 'jahreswechsel', 'und', 'hoffe,', 'daß', 'sie', 'schöne', 'ferien', 'hatten.'], ['wie', 'sie', 'feststellen', 'konnten,', 'ist', 'der', 'gefürchtete', '\"millenium-bug', '\"', 'nicht', 'eingetreten.', 'doch', 'sind', 'bürger', 'einiger', 'unserer', 'mitgliedstaaten', 'opfer', 'von', 'schrecklichen', 'naturkatastrophen', 'geworden.'], ['im', 'parlament', 'besteht', 'der', 'wunsch', 'nach', 'einer', 'aussprache', 'im', 'verlauf', 'dieser', 'sitzungsperiode', 'in', 'den', 'nächsten', 'tagen.'], ['heute', 'möchte', 'ich', 'sie', 'bitten', '-', 'das', 'ist', 'auch', 'der', 'wunsch', 'einiger', 'kolleginnen', 'und', 'kollegen', '-,', 'allen', 'opfern', 'der', 'stürme,', 'insbesondere', 'in', 'den', 'verschiedenen', 'ländern', 'der', 'europäischen', 'union,', 'in', 'einer', 'schweigeminute', 'zu', 'gedenken.'], ['ich', 'bitte', 'sie,', 'sich', 'zu', 'einer', 'schweigeminute', 'zu', 'erheben.'], ['(das', 'parlament', 'erhebt', 'sich', 'zu', 'einer', 'schweigeminute.)'], ['frau', 'präsidentin,', 'zur', 'geschäftsordnung.'], ['wie', 'sie', 'sicher', 'aus', 'der', 'presse', 'und', 'dem', 'fernsehen', 'wissen,', 'gab', 'es', 'in', 'sri', 'lanka', 'mehrere', 'bombenexplosionen', 'mit', 'zahlreichen', 'toten.'], ['zu', 'den', 'attentatsopfern,', 'die', 'es', 'in', 'jüngster', 'zeit', 'in', 'sri', 'lanka', 'zu', 'beklagen', 'gab,', 'zählt', 'auch', 'herr', 'kumar', 'ponnambalam,', 'der', 'dem', 'europäischen', 'parlament', 'erst', 'vor', 'wenigen', 'monaten', 'einen', 'besuch', 'abgestattet', 'hatte.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A.2.1\n",
    "def tok_lower(text:RDD)->RDD:\n",
    "    return text.map(lambda l:l.lower()).map(lambda l:l.split(\" \"))\n",
    "\n",
    "lines_de_lt=tok_lower(lines_de)\n",
    "lines_en_lt=tok_lower(lines_en)\n",
    "\n",
    "print(f\"num lines in the german version: {lines_de_lt.count()}\")\n",
    "print(f\"num lines in the english version: {lines_en_lt.count()}\")\n",
    "\n",
    "# A.2.2\n",
    "print(lines_de_lt.take(10))\n",
    "print(lines_en_lt.take(10))\n",
    "\n",
    "# A.2.3\n",
    "assert lines_de_lt.count()==lines_en_lt.count(), \\\n",
    "    f\"number of lines differ between english and german after pre-processing\" \\\n",
    "    f\"(en:{lines_en.count()}, de:{lines_de.count()})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbf2a8-b0ec-470a-9265-1b24a07e8e8b",
   "metadata": {},
   "source": [
    "## Question A.3\n",
    "A.3.1 Use Spark to compute the 10 most frequently according words in the English language\n",
    "corpus. Repeat for the other language.\n",
    "A.3.2 Verify that your results are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05af062e-51ea-403f-9613-07e8da7e2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(rdd:RDD)->RDD:\n",
    "    \"\"\"\n",
    "    rdd must consist of lines already split into words\n",
    "\n",
    "    1) flatten sentences (which are already split into lists of words) into one huge list of words\n",
    "    2) remove zero length strings\n",
    "    3) group by word (i.e. create one list of all occurences of each word)\n",
    "    4) map each list of words using the len function, effectively counting the number of occurences of this word\n",
    "    5) sort in descending order so that the first word is the most common word\n",
    "    \"\"\"\n",
    "    return rdd \\\n",
    "        .flatMap(lambda x:x) \\\n",
    "        .filter(lambda x:len(x)>0) \\\n",
    "        .groupBy(lambda x:x) \\\n",
    "        .mapValues(len) \\\n",
    "        .sortBy(lambda x:x[1],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebb81a7d-3425-4018-a993-8a4329f1bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "de_word_freq=get_word_count(lines_de_lt)\n",
    "en_word_freq=get_word_count(lines_en_lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75543a10-8ab0-4637-af64-c2e14a257405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "de_word_freq_sample=de_word_freq.take(10)\n",
    "en_word_freq_sample=en_word_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a18291-628c-4d3b-901c-279576444d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most used german words: die, der, und, in, zu, den, wir, für, ich, das\n",
      "10 most used german words: the, of, to, and, in, that, a, is, for, we\n"
     ]
    }
   ],
   "source": [
    "print(f\"10 most used german words: {', '.join([w[0] for w in de_word_freq_sample])}\")\n",
    "print(f\"10 most used german words: {', '.join([w[0] for w in en_word_freq_sample])}\")\n",
    "# results seem reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe20612-3103-410c-8cdf-f0a4049fa38c",
   "metadata": {},
   "source": [
    "## Question A.4\n",
    "A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two\n",
    "languages. Do this by pairing words found on short lines with the same number of words\n",
    "respectively. We (incorrectly) assume the words stay in the same order when translated.\n",
    "Follow this approach. Work with the pair of RDDs you created in question A.2.\n",
    "Hint: make a new pair of RDDs for each step, sv_1, en_1, sv_2, en_2, ...\n",
    "4\n",
    "1. Key the lines by their line number (hint: ZipWithIndex()).\n",
    "2. Swap the key and value - so that the line number is the key.\n",
    "3. Join the two RDDs together according to the line number key, so you have pairs of\n",
    "matching lines.\n",
    "4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "5. Filter to leave only pairs of sentences with a small number of words per sentence,\n",
    "this should give a more reliable translation (you can experiment).\n",
    "6. Filter to leave only pairs of sentences with the same number of words in each\n",
    "sentence.\n",
    "7. For each sentence pair, map so that you pair each (in order) word in the two\n",
    "sentences. We no longer need the line numbers. (hint: use python’s built in zip()\n",
    "function)\n",
    "8. Use reduce to count the number of occurrences of the word-translation-pairs.\n",
    "9. Print some of the most frequently occurring pairs of words.\n",
    "Do your translations seem reasonable? Use a dictionary to check a few (don’t worry, you\n",
    "won’t be marked down for incorrect translations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "585a529d-8972-4ec0-a51e-08d29b338011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def infer_translation(rdd_0:RDD,rdd_1:RDD)->RDD:\n",
    "    # 1.\n",
    "    rdd_0=rdd_0.zipWithIndex()\n",
    "    rdd_1=rdd_1.zipWithIndex()\n",
    "    # 2.\n",
    "    rdd_0=rdd_0.map(lambda x:(x[1],x[0]))\n",
    "    rdd_1=rdd_1.map(lambda x:(x[1],x[0]))\n",
    "    # 3.\n",
    "    rdd=rdd_0.join(rdd_1)\n",
    "    # 4.\n",
    "    rdd=rdd.filter(lambda x:len(x[1][0])>0 and len(x[1][1])>0)\n",
    "    # 5.\n",
    "    max_sentence_length=20\n",
    "    rdd=rdd.filter(lambda x:len(x[1][0])<=max_sentence_length and len(x[1][1])<=max_sentence_length)\n",
    "    # 6.\n",
    "    rdd=rdd.filter(lambda x:len(x[1][0])==len(x[1][1]))\n",
    "    # 7.\n",
    "    rdd=rdd.map(lambda x:tuple(zip(*x[1])))\n",
    "    # 8.\n",
    "    # instruction say to use reduce, but I don't know how reduce would be the natural way to solve this\n",
    "    rdd=rdd \\\n",
    "        .flatMap(lambda x:x) \\\n",
    "        .groupBy(lambda x:x) \\\n",
    "        .mapValues(len) \\\n",
    "        .sortBy(lambda x:x[1],ascending=False)\n",
    "    return rdd\n",
    "\n",
    "# 9. (view the results)\n",
    "word_translations_de_en=infer_translation(lines_de_lt, lines_en_lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d61a895b-a761-4810-91f4-21b0c8d35db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top n most common word pairings:\n",
      "de : die =?= the : en (24647)\n",
      "de : ist =?= is : en (18050)\n",
      "de : ich =?= i : en (14071)\n",
      "de : wir =?= we : en (13066)\n",
      "de : und =?= and : en (12336)\n",
      "de : der =?= the : en (12156)\n",
      "de : der =?= of : en (5292)\n",
      "de : herr =?= mr : en (5284)\n",
      "de : in =?= in : en (5149)\n",
      "de : aussprache =?= debate : en (4750)\n",
      "de : es =?= it : en (4450)\n",
      "de : nicht =?= not : en (4406)\n",
      "de : das =?= the : en (4365)\n",
      "de : geschlossen. =?= closed. : en (4340)\n",
      "de : das =?= that : en (4186)\n",
      "de : dass =?= that : en (4120)\n",
      "de : eine =?= a : en (3717)\n",
      "de :    =?=    : en (3599)\n",
      "de : für =?= for : en (3558)\n",
      "de : ein =?= a : en (3435)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"top n most common word pairings:\",\n",
    "    *[\n",
    "        f\"de : {w_de} =?= {w_en} : en ({wp_cnt})\"\n",
    "        for ((w_de,w_en),wp_cnt)\n",
    "        in word_translations_de_en.take(20)\n",
    "    ],\n",
    "    sep=\"\\n\"\n",
    ")\n",
    "# these are all sensible (even though one of them is just \" \", which in fact does also mean \" \" in german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25551c-67ff-49e6-af5a-6e9abe2818d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
